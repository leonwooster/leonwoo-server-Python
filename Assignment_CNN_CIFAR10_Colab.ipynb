{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment_CNN_CIFAR10_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonwooster/leonwoo-server-Python/blob/master/Assignment_CNN_CIFAR10_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW9Bg8A-JGaY",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">Assignment: Implement a CNN for Image Classification on CIFAR10 dataset</font>\n",
        "\n",
        "We have seen how to implement a CNN (LeNet5 and LeNet with the batch norm) in the last section. We used MNIST and Fashion MNIST dataset which are grayscale or single channel datasets. In this assignment, you will implement a CNN Model ( similar to LeNet ) for classifying objects in the `CIFAR10` dataset. \n",
        "\n",
        "The CIFAR10 dataset has the following properties\n",
        "1. It has `10` classes.  \n",
        "1. It has colored images, so it has `3-channels`. \n",
        "1. The image shape is `32 x 32`.\n",
        "\n",
        "Samples of CIFAR10- dataset ([source](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html?highlight=cifar)):\n",
        "\n",
        "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w3_cirar10.png\" width=700>\n",
        "\n",
        "\n",
        "# <font color='blue'>Marking Scheme</font>\n",
        "\n",
        "### <font style=\"color:green\">Maximum Points: 30\n",
        "\n",
        "<div>\n",
        "    <table>\n",
        "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Problem</h3></td> <td><h3>Points</h3></td> </tr>\n",
        "        <tr><td><h3>1</h3></td> <td><h3>Implement the CNN Model</h3></td> <td><h3>10</h3></td> </tr>\n",
        "        <tr><td><h3>2</h3></td> <td><h3>Find Mean and Std of Training Data</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>3</h3></td> <td><h3>Model Training & Accuracy</h3></td> <td><h3>15</h3></td> </tr>\n",
        "    </table>\n",
        "</div>\n",
        "\n",
        "\n",
        "# <font color='blue'>Problem Description</font>\n",
        "\n",
        "### <font color='blue'>1. Implement the CNN Model</font>\n",
        "Since the task is to classify objects in a dataset of color images, you need to implement a CNN with 10 output classes. **Also, your model must use `Conv2d`, `BatchNorm2d`, and `ReLU`.** \n",
        "\n",
        "**You need to define the model architecture in the function: `MyModel` ( Step 1 )**\n",
        "\n",
        "Hint: For color images you need to use an input shape that is different than the ones we have been using till now, so that it accepts 3 channel inputs.\n",
        "\n",
        "### <font color='blue'>2. Find Mean and Std of Training Data</font>\n",
        "\n",
        "It is a good practice to normalize the training data. To normalize the data, we need to compute mean and std. As the dataset has colored images, it has `3-channel` (RGB or BGR). We have to find mean and std per channel using training data. \n",
        "\n",
        "**You need to compute the mean and std for the dataset in the function: `get_mean_std_train_data` ( Step 3 )**\n",
        "\n",
        "### <font color='blue'>3. Model Training and Accuracy</font>\n",
        "\n",
        "Once you have defined the model, you can train it. To get better accuracy, you need to play around the training configuration **( Step 5 )** and even the model architecture. You can check the accuracy by running the training loop in `Step 11`.\n",
        "\n",
        "Here are a few hints on how you can improve the accuracy:\n",
        "- Train for longer duration\n",
        "- Try with different learning rate\n",
        "- Try to add more convolutional layers to the architecture\n",
        "- Try to add more nodes in the layers.\n",
        "\n",
        "You need to achieve **75% accuracy** ( See Step11 ) in order to get full marks for this part. \n",
        "\n",
        "**You do not need to implement anything for this, just changing the parameters as mentioned above and running the Notebook will give you the accuracy. ( Step 5 and Step 11 )**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJmbahxxJGah",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "required_training = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6H2Qu9FJGak",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mzyIelRJGao",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "import matplotlib.pyplot as plt  # one of the best graphics library for python"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUx0pTPAJGar",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from typing import Iterable\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOLoiGVlJGav"
      },
      "source": [
        "# <font style=\"color:blue\">1. CNN Model Architecture [10 Points]</font>\n",
        "\n",
        "You have to write the model code here. You can take reference from LeNet code.\n",
        "\n",
        "If you do not get higher accuracy, here are a few hints:\n",
        "- Try to add more convolutional layers to the architecture\n",
        "- Try to add more nodes in the layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJygcZSaJGav"
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # convolution layers\n",
        "        self._body = nn.Sequential(\n",
        "          # input size = (32, 32), output size = (28, 28)\n",
        "          nn.Conv2d(in_channels=3, out_channels=50, kernel_size=2),\n",
        "          nn.BatchNorm2d(50),                \n",
        "          nn.ReLU(inplace=True),                \n",
        "          nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                \n",
        "          # input size = (14, 14), output size = (10, 10)\n",
        "          nn.Conv2d(in_channels=50, out_channels=100, kernel_size=2),\n",
        "          nn.BatchNorm2d(100),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "          nn.Conv2d(in_channels=100, out_channels=200, kernel_size=2),\n",
        "          nn.BatchNorm2d(200),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "          #size(2x2)\n",
        "          nn.Conv2d(in_channels=200, out_channels=500, kernel_size=2),\n",
        "          nn.BatchNorm2d(500),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        # Fully connected layers\n",
        "        self._head = nn.Sequential(\n",
        "          nn.Linear(in_features=500, out_features=300), \n",
        "          nn.ReLU(inplace=True),\n",
        "\n",
        "          nn.Linear(in_features=300, out_features=100), \n",
        "          nn.ReLU(inplace=True),\n",
        "                \n",
        "          nn.Linear(in_features=100, out_features=84),                \n",
        "          nn.ReLU(inplace=True),\n",
        "                \n",
        "          nn.Linear(in_features=84, out_features=10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      # apply feature extractor\n",
        "      x = self._body(x)\n",
        "      x = x.view(x.size()[0], -1)            \n",
        "      x = self._head(x)\n",
        "      return x                    "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq46PTjsJGaz",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">2. Display the Network</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDEEFkVoJGa0",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "outputId": "ea9d4c8a-2c1d-4b7c-d1c9-008699e9bf92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "my_model = MyModel()\n",
        "print(my_model)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyModel(\n",
            "  (_body): Sequential(\n",
            "    (0): Conv2d(3, 50, kernel_size=(2, 2), stride=(1, 1))\n",
            "    (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(50, 100, kernel_size=(2, 2), stride=(1, 1))\n",
            "    (5): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(100, 200, kernel_size=(2, 2), stride=(1, 1))\n",
            "    (9): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Conv2d(200, 500, kernel_size=(2, 2), stride=(1, 1))\n",
            "    (13): BatchNorm2d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU(inplace=True)\n",
            "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (_head): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=300, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=300, out_features=100, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=100, out_features=84, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qR75AnbJGa3",
        "lines_to_next_cell": 2,
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">3. Find Mean and STD of CIFAR10 Data [5 Points]</font>\n",
        "\n",
        "Function **`get_mean_std_train_data`** should `return` `mean` and `std` of training data. You can refer to the code used in the previous section for finding the mean and std of the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW7B7G5cJGa4",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "def get_mean_std_train_data(data_root):    \n",
        "        'inputs[0][0] = R, inputs[0][1] = G, inputs[0][2] = B'\n",
        "        'inputs[0][0][31][31] = Batch 0, R channel at row 31 and column 31 (e.g. tensor(0.4824))'\n",
        "        '32 x 32 image size'\n",
        "        train_transform = transforms.Compose([transforms.ToTensor()])\n",
        "        train_set = datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_transform)    \n",
        "        print('%d training samples.' % len(train_set))\n",
        "        \n",
        "        #placeholder for 3 dim \n",
        "        mean = np.array([0.5, 0.5, 0.5])\n",
        "        std = np.array([0.5, 0.5, 0.5])    \n",
        "\n",
        "        loader = torch.utils.data.DataLoader(train_set, batch_size=200, num_workers=2)\n",
        "        h , w = 0, 0        \n",
        "        batch_end = 0\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        chsum = None\n",
        "        for batch_idx, (inputs, targets) in enumerate(loader): #inputs = 3 dim image data/matrix\n",
        "            inputs = inputs.to(device)\n",
        "            batch_end = batch_idx\n",
        "            if batch_idx == 0:\n",
        "                h, w = inputs.size(2), inputs.size(3) #[B,C,W,H]            \n",
        "                chsum = inputs.sum(dim=(0,2,3), keepdim=True) #first dim = batch, second dim= RGB, third = row, fourth = col\n",
        "            else:\n",
        "                chsum += inputs.sum(dim=(0,2,3), keepdim=True)\n",
        "\n",
        "        mean = chsum/len(train_set)/h/w\n",
        "        print(\"Batch total: %d\" % batch_end) #250 batches for 50,000 images with each batch 200 images\n",
        "        print(\"mean_o: %s\" % mean)\n",
        "\n",
        "        chsum = None    \n",
        "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
        "            inputs = inputs.to(device)\n",
        "            if batch_idx == 0:\n",
        "                chsum = (inputs - mean).pow(2).sum(dim=(0,2,3), keepdim=True)\n",
        "            else:\n",
        "                chsum += (inputs - mean).pow(2).sum(dim=(0,2,3), keepdim=True)\n",
        "        std = torch.sqrt(chsum/(len(train_set) * h * w - 1))\n",
        "        print(\"std_o: %s\" % std)\n",
        "\n",
        "        mean = torch.reshape(mean, (1,3))\n",
        "        std = torch.reshape(std, (1,3))\n",
        "\n",
        "        print(\"mean: %s\" % mean)\n",
        "        print (\"std: %s\" % std)\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsnJTJNXJGa7",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def get_data(batch_size, data_root, num_workers=1):\n",
        "    \n",
        "    \n",
        "    try:\n",
        "        mean, std = get_mean_std_train_data(data_root)\n",
        "        assert len(mean) == len(std) == 3\n",
        "    except:\n",
        "        mean = np.array([0.5, 0.5, 0.5])\n",
        "        std = np.array([0.5, 0.5, 0.5])\n",
        "        \n",
        "    \n",
        "    train_test_transforms = transforms.Compose([\n",
        "        # this re-scale image tensor values between 0-1. image_tensor /= 255\n",
        "        transforms.ToTensor(),\n",
        "        # subtract mean and divide by variance.\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "    \n",
        "    # train dataloader\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root=data_root, train=True, download=False, transform=train_test_transforms),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    \n",
        "    # test dataloader\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root=data_root, train=False, download=False, transform=train_test_transforms),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03-ZuoXfJGa-",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">4. System Configuration</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4LsyamkJGa_",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "@dataclass\n",
        "class SystemConfiguration:\n",
        "    '''\n",
        "    Describes the common system setting needed for reproducible training\n",
        "    '''\n",
        "    seed: int = 42  # seed number to set the state of all random number generators\n",
        "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
        "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIIgMKe8JGbC",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">5. Training Configuration [15 Points]</font>\n",
        "All training parameters are defined here. So, \n",
        "This is where you can improve your accuracy, apart from improving the architecture. \n",
        "\n",
        "Here are a few hints on how you can improve the accuracy:\n",
        "- Train for longer duration\n",
        "- Try with different learning rate\n",
        "\n",
        "**You need to achieve 75% accuracy in order to get full marks for this part.**\n",
        "\n",
        "**You will see the effect of these changes when you run Step 11**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qESDffIJGbD",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "@dataclass\n",
        "class TrainingConfiguration:\n",
        "        '''\n",
        "        Describes configuration of the training process\n",
        "        '''\n",
        "        batch_size: int = 16  # amount of data to pass through the network at each forward-backward iteration\n",
        "        epochs_count: int = 5  # number of times the whole dataset will be passed through the network\n",
        "        learning_rate: float = 0.1  # determines the speed of network's weights update\n",
        "            \n",
        "        log_interval: int = 100  # how many batches to wait between logging training status\n",
        "        test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
        "        data_root: str = \"../resource/lib/publicdata/images\"  # folder to save data\n",
        "        num_workers: int = 30  # number of concurrent processes using to prepare data\n",
        "        device: str = 'cuda'  # device to use for training.\n",
        "        # update changed parameters in blow coding block.\n",
        "        # Please do not change \"data_root\" \n",
        "        \n",
        "        momentum: float = 0.9\n",
        "        eps: float = 1e-05\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAQE-iZMJGbG",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">6. System Setup</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdl_GzCqJGbG",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def setup_system(system_config: SystemConfiguration) -> None:\n",
        "    torch.manual_seed(system_config.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
        "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3OIPd3IJGbJ",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">7. Training</font>\n",
        "We are familiar with the training pipeline used in PyTorch. The following steps are performed in the code below:\n",
        "\n",
        "1. Send the data to the required device ( CPU/GPU )\n",
        "1. Make a forward pass using the forward method.\n",
        "1. Find the loss using the Cross_Entropy function.\n",
        "1. Find the gradients using the backward function.\n",
        "1. Update the weights using the optimizer.\n",
        "1. Find the accuracy of the model\n",
        "\n",
        "Repeat the above for the specified number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EEHzywxJGbK",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def train(\n",
        "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
        "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
        ") -> None:\n",
        "    \n",
        "    # change model in training mood\n",
        "    model.train()\n",
        "    \n",
        "    # to get batch loss\n",
        "    batch_loss = np.array([])\n",
        "    \n",
        "    # to get batch accuracy\n",
        "    batch_acc = np.array([])\n",
        "        \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        \n",
        "        # clone target\n",
        "        indx_target = target.clone()\n",
        "        # send data to device (its is medatory if GPU has to be used)\n",
        "        data = data.to(train_config.device)\n",
        "        # send target to device\n",
        "        target = target.to(train_config.device)\n",
        "\n",
        "        # reset parameters gradient to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass to the model\n",
        "        output = model(data)\n",
        "        \n",
        "        # cross entropy loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        \n",
        "        # find gradients w.r.t training parameters\n",
        "        loss.backward()\n",
        "        # Update parameters using gardients\n",
        "        optimizer.step()\n",
        "        \n",
        "        batch_loss = np.append(batch_loss, [loss.item()])\n",
        "        \n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "            \n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]  \n",
        "                        \n",
        "        # correct prediction\n",
        "        correct = pred.cpu().eq(indx_target).sum()\n",
        "            \n",
        "        # accuracy\n",
        "        acc = float(correct) / float(len(data))\n",
        "        \n",
        "        batch_acc = np.append(batch_acc, [acc])\n",
        "\n",
        "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:              \n",
        "            print(\n",
        "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
        "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
        "                )\n",
        "            )\n",
        "            \n",
        "    epoch_loss = batch_loss.mean()\n",
        "    epoch_acc = batch_acc.mean()\n",
        "    return epoch_loss, epoch_acc"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQD5e4MvJGbN",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">8. Validation</font>\n",
        "\n",
        "After every few epochs **`validation`** will be called with the `trained model` and `test_loader` to get validation loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfYIabotJGbO",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def validate(\n",
        "    train_config: TrainingConfiguration,\n",
        "    model: nn.Module,\n",
        "    test_loader: torch.utils.data.DataLoader,\n",
        ") -> float:\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    count_corect_predictions = 0\n",
        "    for data, target in test_loader:\n",
        "        indx_target = target.clone()\n",
        "        data = data.to(train_config.device)\n",
        "        \n",
        "        target = target.to(train_config.device)\n",
        "        \n",
        "        output = model(data)\n",
        "        # add loss for each mini batch\n",
        "        test_loss += F.cross_entropy(output, target).item()\n",
        "        \n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "        \n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1] \n",
        "        \n",
        "        # add correct prediction count\n",
        "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "    # average over number of mini-batches\n",
        "    test_loss = test_loss / len(test_loader)  \n",
        "    \n",
        "    # average over number of dataset\n",
        "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
        "    \n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
        "        )\n",
        "    )\n",
        "    return test_loss, accuracy/100.0"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kllVaLMYJGbV",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">9. Saving the Model</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX_5qui4JGbW",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def save_model(model, device, model_dir='models', model_file_name='cifar10_cnn_model.pt'):\n",
        "    \n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    model_path = os.path.join(model_dir, model_file_name)\n",
        "\n",
        "    # make sure you transfer the model to cpu.\n",
        "    if device == 'cuda':\n",
        "        model.to('cpu')\n",
        "\n",
        "    # save the state_dict\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    \n",
        "    if device == 'cuda':\n",
        "        model.to('cuda')\n",
        "    \n",
        "    return"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb-Z4XiHJGbZ",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">10. Main</font>\n",
        "\n",
        "In this section of code, we use the configuration parameters defined above and start the training. Here are the important actions being taken in the code below:\n",
        "\n",
        "1. Set up system parameters like CPU/GPU, number of threads etc\n",
        "1. Load the data using dataloaders\n",
        "1. Create an instance of the LeNet model\n",
        "1. Specify optimizer to use.\n",
        "1. Set up variables to track loss and accuracy and start training.\n",
        "1. If loss decreases, saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOKDxgpfJGbZ",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def main(system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n",
        "    \n",
        "    # system configuration\n",
        "    setup_system(system_configuration)\n",
        "\n",
        "    # batch size\n",
        "    batch_size_to_set = training_configuration.batch_size\n",
        "    # num_workers\n",
        "    num_workers_to_set = training_configuration.num_workers\n",
        "    # epochs\n",
        "    epoch_num_to_set = training_configuration.epochs_count\n",
        "\n",
        "    # if GPU is available use training config, \n",
        "    # else lowers batch_size, num_workers and epochs count\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        num_workers_to_set = 2\n",
        "\n",
        "    # data loader\n",
        "    train_loader, test_loader = get_data(\n",
        "        batch_size=training_configuration.batch_size,\n",
        "        data_root=training_configuration.data_root,\n",
        "        num_workers=num_workers_to_set\n",
        "    )\n",
        "    \n",
        "    # Update training configuration\n",
        "    training_configuration = TrainingConfiguration(\n",
        "        device=device,\n",
        "        num_workers=num_workers_to_set\n",
        "    )\n",
        "\n",
        "    # initiate model\n",
        "    model = MyModel()\n",
        "\n",
        "    print(\"Training on {}\".format(training_configuration.device))\n",
        "        \n",
        "    # send model to device (GPU/CPU)\n",
        "    model.to(training_configuration.device)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=training_configuration.learning_rate\n",
        "    )\n",
        "\n",
        "    best_loss = torch.tensor(np.inf)\n",
        "    best_accuracy = torch.tensor(0)\n",
        "    \n",
        "    # epoch train/test loss\n",
        "    epoch_train_loss = np.array([])\n",
        "    epoch_test_loss = np.array([])\n",
        "    \n",
        "    # epch train/test accuracy\n",
        "    epoch_train_acc = np.array([])\n",
        "    epoch_test_acc = np.array([])\n",
        "    \n",
        "    # trainig time measurement\n",
        "    t_begin = time.time()\n",
        "    for epoch in range(training_configuration.epochs_count):\n",
        "        \n",
        "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
        "        \n",
        "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
        "        \n",
        "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
        "\n",
        "        elapsed_time = time.time() - t_begin\n",
        "        speed_epoch = elapsed_time / (epoch + 1)\n",
        "        speed_batch = speed_epoch / len(train_loader)\n",
        "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
        "        \n",
        "        print(\n",
        "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
        "                elapsed_time, speed_epoch, speed_batch, eta\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if epoch % training_configuration.test_interval == 0:\n",
        "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
        "            \n",
        "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
        "        \n",
        "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
        "            \n",
        "            if current_loss < best_loss:\n",
        "                best_loss = current_loss\n",
        "            \n",
        "            if current_accuracy > best_accuracy:\n",
        "                best_accuracy = current_accuracy\n",
        "                print('Accuracy improved, saving the model.\\n')\n",
        "                save_model(model, device)\n",
        "            \n",
        "                \n",
        "    print(\"Total time: {:.2f}, Best Loss: {:.3f}, Best Accuracy: {:.3f}\".format(time.time() - t_begin, best_loss, \n",
        "                                                                                best_accuracy))\n",
        "    \n",
        "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zll4UE--JGbd"
      },
      "source": [
        "# <font style=\"color:blue\">Step 11: Start Training</font>\n",
        "This is where you start the training. You may see that the training does not converge or does not give a good accuracy. You need to change \n",
        "- In Step 1: the network architecture and add a few more layers or more nodes to the already existing layers\n",
        "- In Step 5: training parameters such as learning rate or batch_size or epochs so that the network converges or run the network for longer so that it gets more time to fit the data\n",
        "\n",
        "**You need to make sure that the accuracy at the end is at least 75%.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X13F4qQCJGbe",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "scrolled": true,
        "outputId": "f5bd494e-de37-4546-9f97-d5e7878ba402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if required_training:\n",
        "    model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "50000 training samples.\n",
            "Batch total: 249\n",
            "mean_o: tensor([[[[0.4914]],\n",
            "\n",
            "         [[0.4822]],\n",
            "\n",
            "         [[0.4465]]]], device='cuda:0')\n",
            "std_o: tensor([[[[0.2470]],\n",
            "\n",
            "         [[0.2435]],\n",
            "\n",
            "         [[0.2616]]]], device='cuda:0')\n",
            "mean: tensor([[0.4914, 0.4822, 0.4465]], device='cuda:0')\n",
            "std: tensor([[0.2470, 0.2435, 0.2616]], device='cuda:0')\n",
            "Training on cuda\n",
            "Train Epoch: 0 [1600/50000] Loss: 1.823136 Acc: 0.4375\n",
            "Train Epoch: 0 [3200/50000] Loss: 1.641283 Acc: 0.3125\n",
            "Train Epoch: 0 [4800/50000] Loss: 1.491687 Acc: 0.3750\n",
            "Train Epoch: 0 [6400/50000] Loss: 1.662298 Acc: 0.3125\n",
            "Train Epoch: 0 [8000/50000] Loss: 1.659791 Acc: 0.3750\n",
            "Train Epoch: 0 [9600/50000] Loss: 1.580487 Acc: 0.3750\n",
            "Train Epoch: 0 [11200/50000] Loss: 1.953691 Acc: 0.0625\n",
            "Train Epoch: 0 [12800/50000] Loss: 1.715275 Acc: 0.4375\n",
            "Train Epoch: 0 [14400/50000] Loss: 1.229206 Acc: 0.3750\n",
            "Train Epoch: 0 [16000/50000] Loss: 1.565090 Acc: 0.5625\n",
            "Train Epoch: 0 [17600/50000] Loss: 1.317900 Acc: 0.5625\n",
            "Train Epoch: 0 [19200/50000] Loss: 1.901592 Acc: 0.4375\n",
            "Train Epoch: 0 [20800/50000] Loss: 1.030173 Acc: 0.4375\n",
            "Train Epoch: 0 [22400/50000] Loss: 1.095672 Acc: 0.5625\n",
            "Train Epoch: 0 [24000/50000] Loss: 1.258535 Acc: 0.6250\n",
            "Train Epoch: 0 [25600/50000] Loss: 1.457779 Acc: 0.5000\n",
            "Train Epoch: 0 [27200/50000] Loss: 1.708154 Acc: 0.5000\n",
            "Train Epoch: 0 [28800/50000] Loss: 1.192477 Acc: 0.5625\n",
            "Train Epoch: 0 [30400/50000] Loss: 1.178827 Acc: 0.5625\n",
            "Train Epoch: 0 [32000/50000] Loss: 1.246679 Acc: 0.6250\n",
            "Train Epoch: 0 [33600/50000] Loss: 0.922869 Acc: 0.7500\n",
            "Train Epoch: 0 [35200/50000] Loss: 1.743316 Acc: 0.5000\n",
            "Train Epoch: 0 [36800/50000] Loss: 1.382735 Acc: 0.5000\n",
            "Train Epoch: 0 [38400/50000] Loss: 1.097988 Acc: 0.5625\n",
            "Train Epoch: 0 [40000/50000] Loss: 1.597293 Acc: 0.5625\n",
            "Train Epoch: 0 [41600/50000] Loss: 1.307086 Acc: 0.6875\n",
            "Train Epoch: 0 [43200/50000] Loss: 0.852197 Acc: 0.6875\n",
            "Train Epoch: 0 [44800/50000] Loss: 0.905373 Acc: 0.6250\n",
            "Train Epoch: 0 [46400/50000] Loss: 1.387399 Acc: 0.5000\n",
            "Train Epoch: 0 [48000/50000] Loss: 1.994151 Acc: 0.1875\n",
            "Train Epoch: 0 [49600/50000] Loss: 1.225919 Acc: 0.5000\n",
            "Elapsed 27.63s, 27.63 s/epoch, 0.01 s/batch, ets 110.53s\n",
            "\n",
            "Test set: Average loss: 1.0729, Accuracy: 6255/10000 (63%)\n",
            "\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Train Epoch: 1 [1600/50000] Loss: 1.054141 Acc: 0.6875\n",
            "Train Epoch: 1 [3200/50000] Loss: 0.987725 Acc: 0.6250\n",
            "Train Epoch: 1 [4800/50000] Loss: 0.418284 Acc: 0.8750\n",
            "Train Epoch: 1 [6400/50000] Loss: 1.261101 Acc: 0.5625\n",
            "Train Epoch: 1 [8000/50000] Loss: 1.676833 Acc: 0.5000\n",
            "Train Epoch: 1 [9600/50000] Loss: 1.580750 Acc: 0.5000\n",
            "Train Epoch: 1 [11200/50000] Loss: 0.982423 Acc: 0.6875\n",
            "Train Epoch: 1 [12800/50000] Loss: 1.049817 Acc: 0.5625\n",
            "Train Epoch: 1 [14400/50000] Loss: 0.766449 Acc: 0.8750\n",
            "Train Epoch: 1 [16000/50000] Loss: 1.094947 Acc: 0.7500\n",
            "Train Epoch: 1 [17600/50000] Loss: 1.325522 Acc: 0.4375\n",
            "Train Epoch: 1 [19200/50000] Loss: 1.692781 Acc: 0.5000\n",
            "Train Epoch: 1 [20800/50000] Loss: 1.065637 Acc: 0.6875\n",
            "Train Epoch: 1 [22400/50000] Loss: 0.974492 Acc: 0.6250\n",
            "Train Epoch: 1 [24000/50000] Loss: 0.861796 Acc: 0.6250\n",
            "Train Epoch: 1 [25600/50000] Loss: 1.494050 Acc: 0.5625\n",
            "Train Epoch: 1 [27200/50000] Loss: 1.102264 Acc: 0.6250\n",
            "Train Epoch: 1 [28800/50000] Loss: 0.995897 Acc: 0.6250\n",
            "Train Epoch: 1 [30400/50000] Loss: 0.984655 Acc: 0.5625\n",
            "Train Epoch: 1 [32000/50000] Loss: 1.020066 Acc: 0.8125\n",
            "Train Epoch: 1 [33600/50000] Loss: 0.823305 Acc: 0.6875\n",
            "Train Epoch: 1 [35200/50000] Loss: 0.891502 Acc: 0.6875\n",
            "Train Epoch: 1 [36800/50000] Loss: 1.217656 Acc: 0.5000\n",
            "Train Epoch: 1 [38400/50000] Loss: 1.204046 Acc: 0.6250\n",
            "Train Epoch: 1 [40000/50000] Loss: 1.446340 Acc: 0.5000\n",
            "Train Epoch: 1 [41600/50000] Loss: 1.321771 Acc: 0.5625\n",
            "Train Epoch: 1 [43200/50000] Loss: 1.121530 Acc: 0.6250\n",
            "Train Epoch: 1 [44800/50000] Loss: 0.340317 Acc: 0.9375\n",
            "Train Epoch: 1 [46400/50000] Loss: 0.543389 Acc: 0.8750\n",
            "Train Epoch: 1 [48000/50000] Loss: 1.081634 Acc: 0.6875\n",
            "Train Epoch: 1 [49600/50000] Loss: 0.753093 Acc: 0.7500\n",
            "Elapsed 59.89s, 29.95 s/epoch, 0.01 s/batch, ets 89.84s\n",
            "\n",
            "Test set: Average loss: 0.8864, Accuracy: 6962/10000 (70%)\n",
            "\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Train Epoch: 2 [1600/50000] Loss: 0.505575 Acc: 0.7500\n",
            "Train Epoch: 2 [3200/50000] Loss: 0.716923 Acc: 0.6875\n",
            "Train Epoch: 2 [4800/50000] Loss: 0.988296 Acc: 0.6250\n",
            "Train Epoch: 2 [6400/50000] Loss: 0.656843 Acc: 0.7500\n",
            "Train Epoch: 2 [8000/50000] Loss: 0.766963 Acc: 0.6250\n",
            "Train Epoch: 2 [9600/50000] Loss: 0.514475 Acc: 0.8750\n",
            "Train Epoch: 2 [11200/50000] Loss: 0.800867 Acc: 0.6250\n",
            "Train Epoch: 2 [12800/50000] Loss: 1.134895 Acc: 0.5000\n",
            "Train Epoch: 2 [14400/50000] Loss: 1.617170 Acc: 0.6250\n",
            "Train Epoch: 2 [16000/50000] Loss: 0.526664 Acc: 0.8125\n",
            "Train Epoch: 2 [17600/50000] Loss: 0.986179 Acc: 0.6875\n",
            "Train Epoch: 2 [19200/50000] Loss: 0.727894 Acc: 0.6875\n",
            "Train Epoch: 2 [20800/50000] Loss: 0.424282 Acc: 0.8125\n",
            "Train Epoch: 2 [22400/50000] Loss: 1.305892 Acc: 0.6250\n",
            "Train Epoch: 2 [24000/50000] Loss: 1.188198 Acc: 0.6250\n",
            "Train Epoch: 2 [25600/50000] Loss: 0.693754 Acc: 0.7500\n",
            "Train Epoch: 2 [27200/50000] Loss: 0.715591 Acc: 0.6875\n",
            "Train Epoch: 2 [28800/50000] Loss: 0.756684 Acc: 0.6875\n",
            "Train Epoch: 2 [30400/50000] Loss: 1.049641 Acc: 0.6250\n",
            "Train Epoch: 2 [32000/50000] Loss: 0.515682 Acc: 0.8125\n",
            "Train Epoch: 2 [33600/50000] Loss: 0.840653 Acc: 0.6250\n",
            "Train Epoch: 2 [35200/50000] Loss: 0.751165 Acc: 0.6875\n",
            "Train Epoch: 2 [36800/50000] Loss: 0.808622 Acc: 0.8125\n",
            "Train Epoch: 2 [38400/50000] Loss: 1.087701 Acc: 0.6250\n",
            "Train Epoch: 2 [40000/50000] Loss: 0.432016 Acc: 0.8750\n",
            "Train Epoch: 2 [41600/50000] Loss: 0.598922 Acc: 0.8125\n",
            "Train Epoch: 2 [43200/50000] Loss: 1.135326 Acc: 0.5000\n",
            "Train Epoch: 2 [44800/50000] Loss: 1.145993 Acc: 0.6250\n",
            "Train Epoch: 2 [46400/50000] Loss: 0.856357 Acc: 0.6875\n",
            "Train Epoch: 2 [48000/50000] Loss: 0.824129 Acc: 0.7500\n",
            "Train Epoch: 2 [49600/50000] Loss: 0.403694 Acc: 0.8125\n",
            "Elapsed 92.06s, 30.69 s/epoch, 0.01 s/batch, ets 61.37s\n",
            "\n",
            "Test set: Average loss: 0.8307, Accuracy: 7217/10000 (72%)\n",
            "\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Train Epoch: 3 [1600/50000] Loss: 0.699793 Acc: 0.8125\n",
            "Train Epoch: 3 [3200/50000] Loss: 0.380936 Acc: 0.8750\n",
            "Train Epoch: 3 [4800/50000] Loss: 0.998083 Acc: 0.6250\n",
            "Train Epoch: 3 [6400/50000] Loss: 0.670383 Acc: 0.7500\n",
            "Train Epoch: 3 [8000/50000] Loss: 0.661831 Acc: 0.8125\n",
            "Train Epoch: 3 [9600/50000] Loss: 0.673607 Acc: 0.6875\n",
            "Train Epoch: 3 [11200/50000] Loss: 1.344310 Acc: 0.5625\n",
            "Train Epoch: 3 [12800/50000] Loss: 0.897459 Acc: 0.7500\n",
            "Train Epoch: 3 [14400/50000] Loss: 0.887269 Acc: 0.5625\n",
            "Train Epoch: 3 [16000/50000] Loss: 0.815070 Acc: 0.7500\n",
            "Train Epoch: 3 [17600/50000] Loss: 0.353034 Acc: 0.8125\n",
            "Train Epoch: 3 [19200/50000] Loss: 0.881939 Acc: 0.6250\n",
            "Train Epoch: 3 [20800/50000] Loss: 0.664668 Acc: 0.8125\n",
            "Train Epoch: 3 [22400/50000] Loss: 0.811152 Acc: 0.6875\n",
            "Train Epoch: 3 [24000/50000] Loss: 0.772145 Acc: 0.6875\n",
            "Train Epoch: 3 [25600/50000] Loss: 0.498504 Acc: 0.8750\n",
            "Train Epoch: 3 [27200/50000] Loss: 0.894822 Acc: 0.6875\n",
            "Train Epoch: 3 [28800/50000] Loss: 0.901003 Acc: 0.6875\n",
            "Train Epoch: 3 [30400/50000] Loss: 0.716885 Acc: 0.7500\n",
            "Train Epoch: 3 [32000/50000] Loss: 0.720696 Acc: 0.7500\n",
            "Train Epoch: 3 [33600/50000] Loss: 0.610020 Acc: 0.8125\n",
            "Train Epoch: 3 [35200/50000] Loss: 0.648971 Acc: 0.7500\n",
            "Train Epoch: 3 [36800/50000] Loss: 0.911356 Acc: 0.6875\n",
            "Train Epoch: 3 [38400/50000] Loss: 0.917939 Acc: 0.7500\n",
            "Train Epoch: 3 [40000/50000] Loss: 0.742295 Acc: 0.6875\n",
            "Train Epoch: 3 [41600/50000] Loss: 0.374159 Acc: 0.8750\n",
            "Train Epoch: 3 [43200/50000] Loss: 0.758678 Acc: 0.6875\n",
            "Train Epoch: 3 [44800/50000] Loss: 0.671351 Acc: 0.8125\n",
            "Train Epoch: 3 [46400/50000] Loss: 1.217985 Acc: 0.5625\n",
            "Train Epoch: 3 [48000/50000] Loss: 0.540930 Acc: 0.6875\n",
            "Train Epoch: 3 [49600/50000] Loss: 1.212994 Acc: 0.6250\n",
            "Elapsed 124.32s, 31.08 s/epoch, 0.01 s/batch, ets 31.08s\n",
            "\n",
            "Test set: Average loss: 0.7732, Accuracy: 7410/10000 (74%)\n",
            "\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Train Epoch: 4 [1600/50000] Loss: 0.514293 Acc: 0.8750\n",
            "Train Epoch: 4 [3200/50000] Loss: 0.374430 Acc: 0.8750\n",
            "Train Epoch: 4 [4800/50000] Loss: 0.969845 Acc: 0.7500\n",
            "Train Epoch: 4 [6400/50000] Loss: 0.387057 Acc: 0.8125\n",
            "Train Epoch: 4 [8000/50000] Loss: 0.505559 Acc: 0.8125\n",
            "Train Epoch: 4 [9600/50000] Loss: 0.978272 Acc: 0.6250\n",
            "Train Epoch: 4 [11200/50000] Loss: 0.355721 Acc: 0.9375\n",
            "Train Epoch: 4 [12800/50000] Loss: 0.312921 Acc: 0.9375\n",
            "Train Epoch: 4 [14400/50000] Loss: 1.091444 Acc: 0.6875\n",
            "Train Epoch: 4 [16000/50000] Loss: 0.951240 Acc: 0.6250\n",
            "Train Epoch: 4 [17600/50000] Loss: 0.621298 Acc: 0.8125\n",
            "Train Epoch: 4 [19200/50000] Loss: 0.719181 Acc: 0.7500\n",
            "Train Epoch: 4 [20800/50000] Loss: 0.878442 Acc: 0.6875\n",
            "Train Epoch: 4 [22400/50000] Loss: 0.549061 Acc: 0.8750\n",
            "Train Epoch: 4 [24000/50000] Loss: 0.675707 Acc: 0.8750\n",
            "Train Epoch: 4 [25600/50000] Loss: 0.773806 Acc: 0.8750\n",
            "Train Epoch: 4 [27200/50000] Loss: 0.667468 Acc: 0.7500\n",
            "Train Epoch: 4 [28800/50000] Loss: 0.699587 Acc: 0.8125\n",
            "Train Epoch: 4 [30400/50000] Loss: 0.484955 Acc: 0.8125\n",
            "Train Epoch: 4 [32000/50000] Loss: 0.516780 Acc: 0.8125\n",
            "Train Epoch: 4 [33600/50000] Loss: 0.416540 Acc: 0.8750\n",
            "Train Epoch: 4 [35200/50000] Loss: 0.515776 Acc: 0.8125\n",
            "Train Epoch: 4 [36800/50000] Loss: 1.135143 Acc: 0.6250\n",
            "Train Epoch: 4 [38400/50000] Loss: 0.873695 Acc: 0.6250\n",
            "Train Epoch: 4 [40000/50000] Loss: 0.703787 Acc: 0.8125\n",
            "Train Epoch: 4 [41600/50000] Loss: 0.692145 Acc: 0.6250\n",
            "Train Epoch: 4 [43200/50000] Loss: 0.854137 Acc: 0.6250\n",
            "Train Epoch: 4 [44800/50000] Loss: 0.460525 Acc: 0.8750\n",
            "Train Epoch: 4 [46400/50000] Loss: 0.249006 Acc: 0.9375\n",
            "Train Epoch: 4 [48000/50000] Loss: 0.745915 Acc: 0.7500\n",
            "Train Epoch: 4 [49600/50000] Loss: 0.512452 Acc: 0.7500\n",
            "Elapsed 156.32s, 31.26 s/epoch, 0.01 s/batch, ets 0.00s\n",
            "\n",
            "Test set: Average loss: 0.7672, Accuracy: 7497/10000 (75%)\n",
            "\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Total time: 160.93, Best Loss: 0.767, Best Accuracy: 0.750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAy0TSpHJGbi",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">12. Plot Loss</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usF5DnpSJGbi",
        "lines_to_next_cell": 2,
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# Plot loss\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "x = range(len(epoch_train_loss))\n",
        "\n",
        "\n",
        "plt.figure\n",
        "plt.plot(x, epoch_train_loss, color='r', label=\"train loss\")\n",
        "plt.plot(x, epoch_test_loss, color='b', label=\"validation loss\")\n",
        "plt.xlabel('epoch no.')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my7EZy1MJGbl",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">13. Plot Accuracy</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_LPC69oJGbm",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# Plot loss\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "x = range(len(epoch_train_loss))\n",
        "\n",
        "\n",
        "plt.figure\n",
        "plt.plot(x, epoch_train_acc, color='r', label=\"train accuracy\")\n",
        "plt.plot(x, epoch_test_acc, color='b', label=\"validation accuracy\")\n",
        "plt.xlabel('epoch no.')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='center right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_q1qekQJGbp",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">14. Loading the Model </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM-icBgDJGbp",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# initialize the model\n",
        "cnn_model = MyModel()\n",
        "\n",
        "models = 'models'\n",
        "\n",
        "model_file_name = 'cifar10_cnn_model.pt'\n",
        "\n",
        "model_path = os.path.join(models, model_file_name)\n",
        "\n",
        "# loading the model and getting model parameters by using load_state_dict\n",
        "cnn_model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF5uuusgJGbs",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">15. Model Prediction</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyAmSgK2JGbs",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def prediction(model, train_config, batch_input):\n",
        "    \n",
        "    # send model to cpu/cuda according to your system configuration\n",
        "    model.to(train_config.device)\n",
        "    \n",
        "    # it is important to do model.eval() before prediction\n",
        "    model.eval()\n",
        "\n",
        "    data = batch_input.to(train_config.device)\n",
        "\n",
        "    output = model(data)\n",
        "\n",
        "    # Score to probability using softmax\n",
        "    prob = F.softmax(output, dim=1)\n",
        "\n",
        "    # get the max probability\n",
        "    pred_prob = prob.data.max(dim=1)[0]\n",
        "    \n",
        "    # get the index of the max probability\n",
        "    pred_index = prob.data.max(dim=1)[1]\n",
        "    \n",
        "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaGRXs4HJGbx",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">16. Perform Inference on sample images </font>\n",
        "\n",
        "For prediction, we need to transform the data in the same way as we have done during training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L5ecPKtJGbx",
        "lines_to_next_cell": 2,
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "batch_size = 5\n",
        "train_config = TrainingConfiguration()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    train_config.device = \"cuda\"\n",
        "else:\n",
        "    train_config.device = \"cpu\"\n",
        "    \n",
        "    \n",
        "\n",
        "# load test data without image transformation\n",
        "test = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root=train_config.data_root, train=False, download=False, \n",
        "                   transform=transforms.functional.to_tensor),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=1\n",
        "    )\n",
        "\n",
        "try:\n",
        "    mean, std = get_mean_std_train_data(data_root)\n",
        "    assert len(mean) == len(std) == 3\n",
        "except:\n",
        "    mean = (0.5, 0.5, 0.5)\n",
        "    std = (0.5, 0.5, 0.5)\n",
        "\n",
        "# load testdata with image transformation\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "test_trans = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root=train_config.data_root, train=False, download=False, transform=image_transforms),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=1\n",
        "    )\n",
        "\n",
        "for data, _ in test_trans:\n",
        "    # pass the loaded model\n",
        "    pred, prob = prediction(cnn_model, train_config, data)\n",
        "    break\n",
        "    \n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (3, 3)\n",
        "for images, label in test:\n",
        "    for i, img in enumerate(images):\n",
        "        img = transforms.functional.to_pil_image(img)\n",
        "        plt.imshow(img)\n",
        "        plt.gca().set_title('Pred: {0}({1:0.2}), Label: {2}'.format(classes[pred[i]], prob[i], classes[label[i]]))\n",
        "        plt.show()\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvMswVmpJGb1"
      },
      "source": [
        "# <font style=\"color:blue\">17. Report your findings</font>\n",
        "- \n",
        "- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTVSVzjFJGb2",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">References</font>\n",
        "\n",
        "1. https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "1. https://pytorch.org/tutorials/beginner/saving_loading_models.html"
      ]
    }
  ]
}